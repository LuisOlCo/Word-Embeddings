{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- First of all we are going to clean the text and prepare it into split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ensayos.txt') as fp:\n",
    "    ff = str(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = set([])\n",
    "for element in ff:\n",
    "    elements.add(element)\n",
    "\n",
    "elements = sorted(list(elements)) # ' ','!','(',')','*','-','.',':',';','?','¡','ª','¿','’'\n",
    "# r'(!|(|)|*|-|.|:|?|’|;|¡|ª|¿|,|\\s)\\s*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = ff.replace('\\\\r','').replace('\\n','').replace('»','').replace('«','').strip().split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [' ','!','(',')','-','.',':','?','’',';','¡','ª','¿',',','\\s','\\s*','\\n']\n",
    "list_of_strings = []\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    \n",
    "    string = []\n",
    "    list_of_elements = []\n",
    "    for element in sentence:\n",
    "    \n",
    "        if element in symbols: # if the element is either a number or a letter then we added to the string component\n",
    "            list_of_elements.append(''.join(string))\n",
    "            string = []\n",
    "            list_of_elements.append(element)\n",
    "        else:\n",
    "            string.append(element)\n",
    "    list_of_elements.append(''.join(string))     \n",
    "    list_of_strings.append(list_of_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in list_of_strings:\n",
    "    string = []\n",
    "    for component in sentence:\n",
    "        if component != ' ' and component !='':\n",
    "            string.append(component)\n",
    "    corpus.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Set of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the elements included in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordDictionary(corpus):\n",
    "    \n",
    "    different_words = set([])\n",
    "    \n",
    "    [different_words.add(word) for sentence in corpus for word in sentence]\n",
    "    num_words = len(different_words)\n",
    "    different_words = sorted(list(different_words))\n",
    "    \n",
    "    return different_words, num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_ocurrenceMatrix(corpus, window = 5):\n",
    "    \n",
    "    words, num_words = wordDictionary(corpus)\n",
    "    \n",
    "    # Create a word dictionary to easily access to the index of each word\n",
    "    word_to_idx = {}\n",
    "    [word_to_idx.update({k:len(word_to_idx)}) for k in words]\n",
    "    \n",
    "    # Initializate the matrix\n",
    "    M = np.zeros((num_words,num_words))\n",
    "    \n",
    "    # Filling the matrix with frequency co-ocurrences between words\n",
    "    # For the matrix formation we are going to considered the rows as the center words and the columns as the contexts word\n",
    "    # Going over each sentence in our corpus\n",
    "    for sentence in corpus:\n",
    "        # t represents the position in the sentence, t is associated with the center of thw window\n",
    "        for t in range(len(sentence)):\n",
    "            # t also is going to tell us which row we are going to update\n",
    "            n_row = word_to_idx[sentence[t]]\n",
    "            # i represents the position on the left and right of our center word we are analysing within the range \n",
    "            # of our window\n",
    "            for i in range(1,window+1):\n",
    "                # if the word is not out of range of the sentence lenght then we will access the column corresponding to\n",
    "                # that word to add one to the frequency \n",
    "                if t - i > -1:\n",
    "                    n_col = word_to_idx[sentence[t-i]]\n",
    "                    M[n_row,n_col] += 1\n",
    "                \n",
    "                if t + i < len(sentence):\n",
    "                    n_col = word_to_idx[sentence[t+i]]\n",
    "                    M[n_row,n_col] += 1\n",
    "    # Returnning the co-ocurrence matrix and the word to index dictionary to easily find the words in the matrix        \n",
    "    return M, word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply algorithm for reducing the dimensions of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducingDim(M, dim = 2):\n",
    "    \n",
    "    n_iters = 10\n",
    "    \n",
    "    svd = TruncatedSVD(dim,algorithm='randomized', n_iter =10)\n",
    "    svd.fit(M)\n",
    "    M_reduced_dim = svd.transform(M)\n",
    "    \n",
    "    return M_reduced_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting word vectors in the reduced dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont want to plot all words, the plot may unreadable\n",
    "def plotting(M_reduced_dim, word_to_idx, represent_words):\n",
    "    plt.figure(figsize=(40,25))\n",
    "    plt.axis([0,20,-4,4])\n",
    "    for k,v in word_to_idx.items():\n",
    "        if k in represent_words:\n",
    "            \n",
    "            plt.scatter(M_reduced_dim[v,0],M_reduced_dim[v,1])\n",
    "            plt.annotate(k, (M_reduced_dim[v,0], M_reduced_dim[v,1]))  \n",
    "            plt.savefig('testplot.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Application of the algorithm on the loaded corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can change the size of the window by passing the argument window with the desired size, by default is 5\n",
    "M, word_to_idx = co_ocurrenceMatrix(corpus)\n",
    "# we can change the size of the dimensional space by passing the argument dim with the desired size, by default is 2\n",
    "M_reduced_dim = reducingDim(M)\n",
    "\n",
    "# Plotting the selected words in the vector space\n",
    "different_words, num_words = wordDictionary(corpus)\n",
    "plotting(M_reduced_dim,word_to_idx,different_words[100:400])\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "#M_lengths = np.linalg.norm(M_reduced_dim, axis=1)\n",
    "#M_normalized = M_reduced_dim / M_lengths[:, np.newaxis] # broadcasting\n",
    "\n",
    "#plotting(M_normalized,word_to_idx,different_words[100:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
